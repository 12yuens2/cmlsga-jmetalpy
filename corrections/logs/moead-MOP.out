Running SLURM prolog script on gold53.cluster.local
===============================================================================
Job started on Mon Aug 19 12:32:14 BST 2024
Job ID          : 6462642
Job name        : moead-MOP.slurm
WorkDir         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/slurm
Command         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/slurm/moead-MOP.slurm
Partition       : serial
Num hosts       : 1
Num cores       : 1
Num of tasks    : 1
Hosts allocated : gold53
Job Output Follows ...
===============================================================================
Running SLURM prolog script on gold53.cluster.local
===============================================================================
Job started on Mon Aug 19 12:32:14 BST 2024
Job ID          : 6462639
Job name        : moead-MOP.slurm
WorkDir         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/slurm
Command         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/slurm/moead-MOP.slurm
Partition       : serial
Num hosts       : 1
Num cores       : 1
Num of tasks    : 1
Hosts allocated : gold53
Job Output Follows ...
===============================================================================
Running SLURM prolog script on gold53.cluster.local
===============================================================================
Job started on Mon Aug 19 12:32:14 BST 2024
Job ID          : 6462644
Job name        : moead-MOP.slurm
WorkDir         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/slurm
Command         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/slurm/moead-MOP.slurm
Partition       : serial
Num hosts       : 1
Num cores       : 1
Num of tasks    : 1
Hosts allocated : gold53
Job Output Follows ...
===============================================================================
Running SLURM prolog script on gold53.cluster.local
===============================================================================
Job started on Mon Aug 19 12:32:14 BST 2024
Job ID          : 6462643
Job name        : moead-MOP.slurm
WorkDir         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/slurm
Command         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/slurm/moead-MOP.slurm
Partition       : serial
Num hosts       : 1
Num cores       : 1
Num of tasks    : 1
Hosts allocated : gold53
Job Output Follows ...
===============================================================================
==============================================================================
Running epilogue script on gold53.

Submit time  : 2024-08-19T12:31:52
Start time   : 2024-08-19T12:32:14
End time     : 2024-08-19T12:49:34
Elapsed time : 00:17:20 (Timelimit=1-23:55:00)

Job ID: 6462643
Array Job ID: 6462639_5
Cluster: i5
User/Group: sy6u19/mm
State: COMPLETED (exit code 0)
Cores: 1
CPU Utilized: 00:17:01
CPU Efficiency: 98.17% of 00:17:20 core-walltime
Job Wall-clock time: 00:17:20
Memory Utilized: 1.04 GB
Memory Efficiency: 13.25% of 7.81 GB

==============================================================================
Running epilogue script on gold53.

Submit time  : 2024-08-19T12:31:52
Start time   : 2024-08-19T12:32:14
End time     : 2024-08-19T12:52:25
Elapsed time : 00:20:11 (Timelimit=1-23:55:00)

Job ID: 6462644
Array Job ID: 6462639_6
Cluster: i5
User/Group: sy6u19/mm
State: COMPLETED (exit code 0)
Cores: 1
CPU Utilized: 00:19:51
CPU Efficiency: 98.35% of 00:20:11 core-walltime
Job Wall-clock time: 00:20:11
Memory Utilized: 1.06 GB
Memory Efficiency: 13.59% of 7.81 GB

==============================================================================
Running epilogue script on gold53.

Submit time  : 2024-08-19T12:31:52
Start time   : 2024-08-19T12:32:14
End time     : 2024-08-19T12:54:29
Elapsed time : 00:22:15 (Timelimit=1-23:55:00)

Job ID: 6462642
Array Job ID: 6462639_4
Cluster: i5
User/Group: sy6u19/mm
State: COMPLETED (exit code 0)
Cores: 1
CPU Utilized: 00:21:51
CPU Efficiency: 98.20% of 00:22:15 core-walltime
Job Wall-clock time: 00:22:15
Memory Utilized: 1.06 GB
Memory Efficiency: 13.60% of 7.81 GB

