Running SLURM prolog script on gold51.cluster.local
===============================================================================
Job started on Sun Sep 29 21:12:51 BST 2024
Job ID          : 6604685
Job name        : U-NSGA-IIIe-non-cont-DASCMOP-6.slurm
WorkDir         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/summary-slurm
Command         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/summary-slurm/U-NSGA-IIIe-non-cont-DASCMOP-6.slurm
Partition       : serial
Num hosts       : 1
Num cores       : 1
Num of tasks    : 1
Hosts allocated : gold51
Job Output Follows ...
===============================================================================
Running SLURM prolog script on gold51.cluster.local
===============================================================================
Job started on Sun Sep 29 21:12:51 BST 2024
Job ID          : 6604686
Job name        : U-NSGA-IIIe-non-cont-DASCMOP-6.slurm
WorkDir         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/summary-slurm
Command         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/summary-slurm/U-NSGA-IIIe-non-cont-DASCMOP-6.slurm
Partition       : serial
Num hosts       : 1
Num cores       : 1
Num of tasks    : 1
Hosts allocated : gold51
Job Output Follows ...
===============================================================================
Running SLURM prolog script on gold51.cluster.local
===============================================================================
Job started on Sun Sep 29 21:12:51 BST 2024
Job ID          : 6604688
Job name        : U-NSGA-IIIe-non-cont-DASCMOP-6.slurm
WorkDir         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/summary-slurm
Command         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/summary-slurm/U-NSGA-IIIe-non-cont-DASCMOP-6.slurm
Partition       : serial
Num hosts       : 1
Num cores       : 1
Num of tasks    : 1
Hosts allocated : gold51
Job Output Follows ...
===============================================================================
Running SLURM prolog script on gold51.cluster.local
===============================================================================
Job started on Sun Sep 29 21:12:51 BST 2024
Job ID          : 6604687
Job name        : U-NSGA-IIIe-non-cont-DASCMOP-6.slurm
WorkDir         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/summary-slurm
Command         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/summary-slurm/U-NSGA-IIIe-non-cont-DASCMOP-6.slurm
Partition       : serial
Num hosts       : 1
Num cores       : 1
Num of tasks    : 1
Hosts allocated : gold51
Job Output Follows ...
===============================================================================
Running SLURM prolog script on gold51.cluster.local
===============================================================================
Job started on Sun Sep 29 21:12:51 BST 2024
Job ID          : 6604689
Job name        : U-NSGA-IIIe-non-cont-DASCMOP-6.slurm
WorkDir         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/summary-slurm
Command         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/summary-slurm/U-NSGA-IIIe-non-cont-DASCMOP-6.slurm
Partition       : serial
Num hosts       : 1
Num cores       : 1
Num of tasks    : 1
Hosts allocated : gold51
Job Output Follows ...
===============================================================================
Running SLURM prolog script on gold52.cluster.local
===============================================================================
Job started on Sun Sep 29 21:12:51 BST 2024
Job ID          : 6604026
Job name        : U-NSGA-IIIe-non-cont-DASCMOP-6.slurm
WorkDir         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/summary-slurm
Command         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/summary-slurm/U-NSGA-IIIe-non-cont-DASCMOP-6.slurm
Partition       : serial
Num hosts       : 1
Num cores       : 1
Num of tasks    : 1
Hosts allocated : gold52
Job Output Follows ...
===============================================================================
/ssdfs/users/sy6u19/data-100000evals-30runs-corrections-normals/U-NSGA-IIIe-non-cont/DASCMOP6(6)
U-NSGA-IIIe-non-cont, DASCMOP6(6)
U-NSGA-IIIe-non-cont, DASCMOP6(6)
/ssdfs/users/sy6u19/data-100000evals-30runs-corrections-normals/U-NSGA-IIIe-non-cont/DASCMOP3(6)
U-NSGA-IIIe-non-cont, DASCMOP3(6)
U-NSGA-IIIe-non-cont, DASCMOP3(6)
/ssdfs/users/sy6u19/data-100000evals-30runs-corrections-normals/U-NSGA-IIIe-non-cont/DASCMOP2(6)
U-NSGA-IIIe-non-cont, DASCMOP2(6)
U-NSGA-IIIe-non-cont, DASCMOP2(6)
/ssdfs/users/sy6u19/data-100000evals-30runs-corrections-normals/U-NSGA-IIIe-non-cont/DASCMOP4(6)
U-NSGA-IIIe-non-cont, DASCMOP4(6)
U-NSGA-IIIe-non-cont, DASCMOP4(6)
/ssdfs/users/sy6u19/data-100000evals-30runs-corrections-normals/U-NSGA-IIIe-non-cont/DASCMOP5(6)
U-NSGA-IIIe-non-cont, DASCMOP5(6)
U-NSGA-IIIe-non-cont, DASCMOP5(6)
/ssdfs/users/sy6u19/data-100000evals-30runs-corrections-normals/U-NSGA-IIIe-non-cont/DASCMOP1(6)
U-NSGA-IIIe-non-cont, DASCMOP1(6)
U-NSGA-IIIe-non-cont, DASCMOP1(6)
==============================================================================
==============================================================================
Running epilogue script on gold51.
Running epilogue script on gold51.


Submit time  : 2024-09-29T19:10:40
Submit time  : 2024-09-29T19:10:40
Start time   : 2024-09-29T21:12:51
Start time   : 2024-09-29T21:12:51
End time     : 2024-09-29T21:12:55
End time     : 2024-09-29T21:12:55
Elapsed time : 00:00:04 Elapsed time : 00:00:04 (Timelimit=1-23:55:00)

(Timelimit=1-23:55:00)

==============================================================================
Running epilogue script on gold51.

Submit time  : 2024-09-29T19:10:40
==============================================================================
Running epilogue script on gold51.

==============================================================================
Running epilogue script on gold51.

Submit time  : 2024-09-29T19:10:40
Start time   : 2024-09-29T21:12:51
Submit time  : 2024-09-29T19:10:40
Start time   : 2024-09-29T21:12:51
Start time   : 2024-09-29T21:12:51
End time     : 2024-09-29T21:12:55
End time     : 2024-09-29T21:12:55
End time     : 2024-09-29T21:12:55
Elapsed time : 00:00:04 Elapsed time : 00:00:04 Job ID: 6604688
Array Job ID: 6604026_4
Cluster: i5
User/Group: sy6u19/mm
State: FAILED (exit code 1)
Cores: 1
CPU Utilized: 00:00:02
CPU Efficiency: 50.00% of 00:00:04 core-walltime
Job Wall-clock time: 00:00:04
Memory Utilized: 2.35 MB
Memory Efficiency: 0.03% of 7.81 GB

Elapsed time : 00:00:04 Job ID: 6604689
Array Job ID: 6604026_5
Cluster: i5
User/Group: sy6u19/mm
State: FAILED (exit code 1)
Cores: 1
CPU Utilized: 00:00:02
CPU Efficiency: 50.00% of 00:00:04 core-walltime
Job Wall-clock time: 00:00:04
Memory Utilized: 2.35 MB
Memory Efficiency: 0.03% of 7.81 GB

(Timelimit=1-23:55:00)
(Timelimit=1-23:55:00)


(Timelimit=1-23:55:00)

Job ID: 6604687
Array Job ID: 6604026_3
Cluster: i5
User/Group: sy6u19/mm
State: FAILED (exit code 1)
Cores: 1
CPU Utilized: 00:00:02
CPU Efficiency: 50.00% of 00:00:04 core-walltime
Job Wall-clock time: 00:00:04
Memory Utilized: 2.35 MB
Memory Efficiency: 0.03% of 7.81 GB

Job ID: 6604686
Array Job ID: 6604026_2
Cluster: i5
User/Group: sy6u19/mm
State: FAILED (exit code 1)
Cores: 1
CPU Utilized: 00:00:02
CPU Efficiency: 50.00% of 00:00:04 core-walltime
Job Wall-clock time: 00:00:04
Memory Utilized: 2.35 MB
Memory Efficiency: 0.03% of 7.81 GB

Job ID: 6604685
Array Job ID: 6604026_1
Cluster: i5
User/Group: sy6u19/mm
State: FAILED (exit code 1)
Cores: 1
CPU Utilized: 00:00:02
CPU Efficiency: 50.00% of 00:00:04 core-walltime
Job Wall-clock time: 00:00:04
Memory Utilized: 2.35 MB
Memory Efficiency: 0.03% of 7.81 GB

