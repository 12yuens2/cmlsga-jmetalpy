Running SLURM prolog script on gold51.cluster.local
Running SLURM prolog script on gold51.cluster.local
===============================================================================
===============================================================================
Job started on Sun Sep 29 21:16:06 BST 2024
Job started on Sun Sep 29 21:16:06 BST 2024
Job ID          : 6604737
Job name        : U-NSGA-IIIe-non-cont-UF.slurm
Job ID          : 6604735
WorkDir         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/summary-slurm
Job name        : U-NSGA-IIIe-non-cont-UF.slurm
Command         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/summary-slurm/U-NSGA-IIIe-non-cont-UF.slurm
WorkDir         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/summary-slurm
Partition       : serial
Command         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/summary-slurm/U-NSGA-IIIe-non-cont-UF.slurm
Num hosts       : 1
Partition       : serial
Num cores       : 1
Num hosts       : 1
Num of tasks    : 1
Num cores       : 1
Hosts allocated : gold51
Num of tasks    : 1
Job Output Follows ...
Hosts allocated : gold51
===============================================================================
Job Output Follows ...
===============================================================================
Running SLURM prolog script on gold51.cluster.local
===============================================================================
Job started on Sun Sep 29 21:16:06 BST 2024
Job ID          : 6604738
Job name        : U-NSGA-IIIe-non-cont-UF.slurm
WorkDir         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/summary-slurm
Command         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/summary-slurm/U-NSGA-IIIe-non-cont-UF.slurm
Partition       : serial
Num hosts       : 1
Num cores       : 1
Num of tasks    : 1
Hosts allocated : gold51
Job Output Follows ...
===============================================================================
Running SLURM prolog script on gold52.cluster.local
===============================================================================
Job started on Sun Sep 29 21:16:06 BST 2024
Job ID          : 6604741
Job name        : U-NSGA-IIIe-non-cont-UF.slurm
WorkDir         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/summary-slurm
Command         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/summary-slurm/U-NSGA-IIIe-non-cont-UF.slurm
Partition       : serial
Num hosts       : 1
Num cores       : 1
Num of tasks    : 1
Hosts allocated : gold52
Job Output Follows ...
===============================================================================
Running SLURM prolog script on gold52.cluster.local
===============================================================================
Job started on Sun Sep 29 21:16:06 BST 2024
Job ID          : 6604740
Job name        : U-NSGA-IIIe-non-cont-UF.slurm
WorkDir         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/summary-slurm
Command         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/summary-slurm/U-NSGA-IIIe-non-cont-UF.slurm
Partition       : serial
Num hosts       : 1
Num cores       : 1
Num of tasks    : 1
Hosts allocated : gold52
Job Output Follows ...
===============================================================================
Running SLURM prolog script on gold51.cluster.local
===============================================================================
Job started on Sun Sep 29 21:16:06 BST 2024
Job ID          : 6604736
Job name        : U-NSGA-IIIe-non-cont-UF.slurm
WorkDir         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/summary-slurm
Command         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/summary-slurm/U-NSGA-IIIe-non-cont-UF.slurm
Partition       : serial
Num hosts       : 1
Num cores       : 1
Num of tasks    : 1
Hosts allocated : gold51
Job Output Follows ...
===============================================================================
Running SLURM prolog script on gold51.cluster.local
===============================================================================
Job started on Sun Sep 29 21:16:06 BST 2024
Job ID          : 6604739
Job name        : U-NSGA-IIIe-non-cont-UF.slurm
WorkDir         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/summary-slurm
Command         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/summary-slurm/U-NSGA-IIIe-non-cont-UF.slurm
Partition       : serial
Num hosts       : 1
Num cores       : 1
Num of tasks    : 1
Hosts allocated : gold51
Job Output Follows ...
===============================================================================
Running SLURM prolog script on gold52.cluster.local
===============================================================================
Job started on Sun Sep 29 21:16:06 BST 2024
Running SLURM prolog script on gold52.cluster.local
Job ID          : 6604742
===============================================================================
Job started on Sun Sep 29 21:16:06 BST 2024
Job ID          : 6604032
Job name        : U-NSGA-IIIe-non-cont-UF.slurm
Job name        : U-NSGA-IIIe-non-cont-UF.slurm
WorkDir         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/summary-slurm
WorkDir         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/summary-slurm
Command         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/summary-slurm/U-NSGA-IIIe-non-cont-UF.slurm
Partition       : serial
Num hosts       : 1
Command         : /mainfs/home/sy6u19/cmlsga-jmetalpy/corrections/summary-slurm/U-NSGA-IIIe-non-cont-UF.slurm
Num cores       : 1
Num of tasks    : 1
Partition       : serial
Hosts allocated : gold52
Job Output Follows ...
Num hosts       : 1
===============================================================================
Num cores       : 1
Num of tasks    : 1
Hosts allocated : gold52
Job Output Follows ...
===============================================================================
/ssdfs/users/sy6u19/data-100000evals-30runs-corrections-normals/U-NSGA-IIIe-non-cont/UF6
U-NSGA-IIIe-non-cont, UF6
U-NSGA-IIIe-non-cont, UF6
/ssdfs/users/sy6u19/data-100000evals-30runs-corrections-normals/U-NSGA-IIIe-non-cont/UF9
U-NSGA-IIIe-non-cont, UF9
U-NSGA-IIIe-non-cont, UF9
/ssdfs/users/sy6u19/data-100000evals-30runs-corrections-normals/U-NSGA-IIIe-non-cont/UF7
U-NSGA-IIIe-non-cont, UF7
U-NSGA-IIIe-non-cont, UF7
/ssdfs/users/sy6u19/data-100000evals-30runs-corrections-normals/U-NSGA-IIIe-non-cont/UF8
U-NSGA-IIIe-non-cont, UF8
U-NSGA-IIIe-non-cont, UF8
==============================================================================
Running epilogue script on gold52.

==============================================================================
Running epilogue script on gold52.

Submit time  : 2024-09-29T19:10:40
Submit time  : 2024-09-29T19:10:40
Start time   : 2024-09-29T21:16:06
Start time   : 2024-09-29T21:16:06
End time     : 2024-09-29T21:16:09
End time     : 2024-09-29T21:16:09
Elapsed time : 00:00:03 Elapsed time : 00:00:03 (Timelimit=1-23:55:00)

==============================================================================
Running epilogue script on gold52.

(Timelimit=1-23:55:00)

Submit time  : 2024-09-29T19:10:40
Start time   : 2024-09-29T21:16:06
End time     : 2024-09-29T21:16:10
Elapsed time : 00:00:04 (Timelimit=1-23:55:00)

Job ID: 6604741
Array Job ID: 6604032_7
Cluster: i5
User/Group: sy6u19/mm
State: FAILED (exit code 1)
Cores: 1
CPU Utilized: 00:00:03
CPU Efficiency: 100.00% of 00:00:03 core-walltime
Job Wall-clock time: 00:00:03
Memory Utilized: 2.37 MB
Memory Efficiency: 0.03% of 7.81 GB

Job ID: 6604740
Array Job ID: 6604032_6
Cluster: i5
User/Group: sy6u19/mm
State: FAILED (exit code 1)
Cores: 1
CPU Utilized: 00:00:03
CPU Efficiency: 100.00% of 00:00:03 core-walltime
Job Wall-clock time: 00:00:03
Memory Utilized: 2.54 MB
Memory Efficiency: 0.03% of 7.81 GB

Job ID: 6604742
Array Job ID: 6604032_8
Cluster: i5
User/Group: sy6u19/mm
State: FAILED (exit code 1)
Cores: 1
CPU Utilized: 00:00:03
CPU Efficiency: 75.00% of 00:00:04 core-walltime
Job Wall-clock time: 00:00:04
Memory Utilized: 2.37 MB
Memory Efficiency: 0.03% of 7.81 GB

/ssdfs/users/sy6u19/data-100000evals-30runs-corrections-normals/U-NSGA-IIIe-non-cont/UF5
U-NSGA-IIIe-non-cont, UF5
U-NSGA-IIIe-non-cont, UF5
/ssdfs/users/sy6u19/data-100000evals-30runs-corrections-normals/U-NSGA-IIIe-non-cont/UF3
U-NSGA-IIIe-non-cont, UF3
U-NSGA-IIIe-non-cont, UF3
/ssdfs/users/sy6u19/data-100000evals-30runs-corrections-normals/U-NSGA-IIIe-non-cont/UF4
U-NSGA-IIIe-non-cont, UF4
U-NSGA-IIIe-non-cont, UF4
/ssdfs/users/sy6u19/data-100000evals-30runs-corrections-normals/U-NSGA-IIIe-non-cont/UF1
U-NSGA-IIIe-non-cont, UF1
U-NSGA-IIIe-non-cont, UF1
/ssdfs/users/sy6u19/data-100000evals-30runs-corrections-normals/U-NSGA-IIIe-non-cont/UF2
U-NSGA-IIIe-non-cont, UF2
U-NSGA-IIIe-non-cont, UF2
==============================================================================
==============================================================================
==============================================================================
==============================================================================
==============================================================================
Running epilogue script on gold51.
Running epilogue script on gold51.
Running epilogue script on gold51.
Running epilogue script on gold51.




Running epilogue script on gold51.

Submit time  : 2024-09-29T19:10:40
Submit time  : 2024-09-29T19:10:40
Submit time  : 2024-09-29T19:10:40
Submit time  : 2024-09-29T19:10:40
Submit time  : 2024-09-29T19:10:40
Start time   : 2024-09-29T21:16:06
Start time   : 2024-09-29T21:16:06
Start time   : 2024-09-29T21:16:06
Start time   : 2024-09-29T21:16:06
Start time   : 2024-09-29T21:16:06
End time     : 2024-09-29T21:16:22
End time     : 2024-09-29T21:16:22
End time     : 2024-09-29T21:16:22
End time     : 2024-09-29T21:16:22
End time     : 2024-09-29T21:16:22
Elapsed time : 00:00:16 Elapsed time : 00:00:16 Elapsed time : 00:00:16 Elapsed time : 00:00:16 Elapsed time : 00:00:16 (Timelimit=1-23:55:00)

(Timelimit=1-23:55:00)

(Timelimit=1-23:55:00)

(Timelimit=1-23:55:00)

(Timelimit=1-23:55:00)

Job ID: 6604736
Array Job ID: 6604032_2
Cluster: i5
User/Group: sy6u19/mm
State: FAILED (exit code 1)
Cores: 1
CPU Utilized: 00:00:04
CPU Efficiency: 25.00% of 00:00:16 core-walltime
Job Wall-clock time: 00:00:16
Memory Utilized: 2.35 MB
Memory Efficiency: 0.03% of 7.81 GB
Job ID: 6604737
Array Job ID: 6604032_3
Cluster: i5
User/Group: sy6u19/mm
State: FAILED (exit code 1)
Cores: 1
CPU Utilized: 00:00:04
CPU Efficiency: 25.00% of 00:00:16 core-walltime
Job Wall-clock time: 00:00:16
Memory Utilized: 2.35 MB
Memory Efficiency: 0.03% of 7.81 GB
Job ID: 6604739
Array Job ID: 6604032_5
Cluster: i5
User/Group: sy6u19/mm
State: FAILED (exit code 1)
Cores: 1
CPU Utilized: 00:00:04
CPU Efficiency: 25.00% of 00:00:16 core-walltime
Job Wall-clock time: 00:00:16
Memory Utilized: 2.35 MB
Memory Efficiency: 0.03% of 7.81 GB

Job ID: 6604735
Array Job ID: 6604032_1
Cluster: i5
User/Group: sy6u19/mm
State: FAILED (exit code 1)
Cores: 1
CPU Utilized: 00:00:04
CPU Efficiency: 25.00% of 00:00:16 core-walltime
Job Wall-clock time: 00:00:16
Memory Utilized: 2.35 MB
Memory Efficiency: 0.03% of 7.81 GB



Job ID: 6604738
Array Job ID: 6604032_4
Cluster: i5
User/Group: sy6u19/mm
State: FAILED (exit code 1)
Cores: 1
CPU Utilized: 00:00:04
CPU Efficiency: 25.00% of 00:00:16 core-walltime
Job Wall-clock time: 00:00:16
Memory Utilized: 2.35 MB
Memory Efficiency: 0.03% of 7.81 GB

